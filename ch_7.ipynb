{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanoridolfi/ML_Mastery_Python/blob/master/ch_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cJM0868TqTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chpater 7b##########################\n",
        "##############################################\n",
        "import pandas as pd\n",
        "from pandas import set_option\n",
        "from csv import reader\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "CSV_url='https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "data=pd.read_csv(CSV_url,header=None,names=names)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lxLhhA_K3c0",
        "colab_type": "text"
      },
      "source": [
        "# Rescale Data\n",
        "\n",
        "When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale. Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like k-Nearest Neighbors. You can rescale your data using scikit-learn using the *MinMaxScaler *class.\n",
        "\n",
        "---\n",
        "\n",
        "MinMax Scaling\n",
        "\n",
        "A function for min-max scaling of pandas DataFrames or NumPy arrays.\n",
        "\n",
        "from mlxtend.preprocessing import MinMaxScaling\n",
        "\n",
        "An alternative approach to Z-score normalization (or standardization) is the so-called Min-Max scaling (often also simply called \"normalization\" - a common cause for ambiguities). In this approach, the data is scaled to a fixed range - usually 0 to 1. The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
        "\n",
        "A Min-Max scaling is typically done via the following equation:\n",
        "\n",
        "Xsc=(X−Xmin)/(Xmax−Xmin)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQtLPciANeOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "X = np.array([[1, 10], [2, 9], [3, 8], \n",
        "              [4, 7], [5, 6], [6, 5]])\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
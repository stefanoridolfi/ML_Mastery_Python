{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Open_source_DL_frameworks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "divl7TK36uPv"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanoridolfi/ML_Mastery_Python/blob/master/Open_source_DL_frameworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF-JFhOG1pyH",
        "colab_type": "text"
      },
      "source": [
        "# Open-Source Frameworks for Deep Learning: an Overview\n",
        "\n",
        "\n",
        "This notebook is part of the 2 hrs talk given at the Univerity of Bologna (DISI), Nuovo Campus Universitario, Via Pavese 50, Cesena, FC the 13th of December 2018, 10-12 am. Remember to include the copyright if you want to use, modify or distribute this notebook! :-) Slides of the talk are available [here](https://docs.google.com/presentation/d/1fbTKtp9xOlCL4JtpiLF39x7Vxq2Z-jgn77CTqcBrwEE/edit?usp=sharing).\n",
        "\n",
        "**Last Update: 11-12-2019**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2jITHxjvZx7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Abstract** : The rise of deep learning over the last decade has led to profound changes in the landscape of the machine learning software stack both for research and production. In this talk we will provide a comprehensive overview of the *open-source deep learning frameworks* landscape with both a theoretical and hands-on approach. After a brief introduction and historical contextualization, we will highlight common features and distinctions of their recent developments. Finally, we will take at deeper look into three of the most used deep learning frameworks today: *Caffe*, *Tensorflow*, *PyTorch*; with practical examples and considerations worth reckoning in the choice of such libraries.\n",
        "\n",
        "**Short Bio** : \"Vincenzo Lomonaco is a Postdoctoral Researcher at the University of Bologna, Italy and President of ContinualAI, a non-profit research organization and the largest open community on Continual/Lifelong Learning for AI. Currently, he is also a Co-founder and Board Member of AI for People and a Research Affiliate at AI Labs. Vincenzo obtained his PhD at UniBo in early 2019 with a dissertation titled \"Continual Learning with Deep Architectures\": a natural continuation of his master's thesis on bio-inspired deep architectures he started in 2014. For more than 3 years he has been working as a teaching assistant for the “Machine Learning” and “Computer Architectures” courses in the Department of Computer Science of Engineering (DISI) in the same university. He has been a visiting research scientist at Purdue University, USA in 2017, at ENSTA ParisTech Grande École, France in 2018 and at Numenta, USA in 2019. His main research interests include open science and ethical AI developments, continual/lifelong learning with deep architectures, multi-task learning, knowledge distillation and transfer, and their applications to embedded systems, robotics and internet-of-things\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pJ2xB48pjgj",
        "colab_type": "text"
      },
      "source": [
        "** Connecting a local runtime**\n",
        "\n",
        "In case resources are not enough for you (no GPU for example), you can always connect another [local runtime](https://research.google.com/colaboratory/local-runtimes.html) or to a [runtime on a Google Compute Engine instance](https://research.google.com/colaboratory/local-runtimes.html). However, this notebook has been designed to run fast enough on simple CPUs so you shouldn't fined any trouble here, using a free *hosted account*.\n",
        "\n",
        "\n",
        "**Requisites to run it locally, outside colab (not recommended)**\n",
        "\n",
        "*   Python 3.x\n",
        "*   Jupyter\n",
        "*   Numpy\n",
        "*   Matplolib\n",
        "*   Pytorch 0.4.0\n",
        "*   Caffe 1.0.0\n",
        "*  Tensorflow 1.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg1m-mau3nim",
        "colab_type": "text"
      },
      "source": [
        "# Hands-on session (45 minutes)\n",
        "\n",
        "In this session we will try to learn an evaluate a Convolutional Neural Networks model on [MNIST](http://yann.lecun.com/exdb/mnist/) using three of the most used deep learning frameworks today: *Caffe*, *Tensorflow*, *PyTorch* with an expected timeframe of 15 minutes for each of them. This will allow us to grasp what it means to train a deep model with such libraries and compare the different Python APIs for this simple use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD97xBZTgyjW",
        "colab_type": "text"
      },
      "source": [
        "## Google Colaboratory\n",
        "\n",
        "First of all, take a moment to look around and discover Google Colab if you haven't before! You can run the commands below to understand how much resources you're using and are still available. Then consider also that you can also connect you Google Drive for additional space or for easily loading your own files. Check out the [official tutorial](https://colab.research.google.com/) of the Google Colaboratory for more information.\n",
        "\n",
        "You can always reset the entire VM with \"*Runtime > Reset all runtime*\" in case of difficulty. Make also sure you're using the GPU or TPU in the same tab (\"*Runtime > Change runtime type*\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmbdpXfPhBeA",
        "colab_type": "code",
        "outputId": "2ddd5620-fa59-47f4-fc44-7da876952896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "!free -m\n",
        "!df -h\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:          13022         539       10446           0        2036       12207\n",
            "Swap:             0           0           0\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         359G   32G  310G  10% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "tmpfs           6.4G   12K  6.4G   1% /var/colab\n",
            "/dev/sda1       365G   37G  329G  10% /opt/bin\n",
            "shm             5.9G  4.0K  5.9G   1% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "Thu Dec 12 10:11:03 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.36       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0VsdEjmhEY7",
        "colab_type": "text"
      },
      "source": [
        "Questions to explore:\n",
        "\n",
        "*   How to connect your Google Drive with Google Colab?\n",
        "*   How to import a new notebook and save it to your GDrive?\n",
        "*  How to use files which are contained in your GDrive?\n",
        "\n",
        "Some tips here: https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDvwfvb9hGvi",
        "colab_type": "text"
      },
      "source": [
        "## Loading the MNIST Benchamark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SbJJIUenz07",
        "colab_type": "text"
      },
      "source": [
        "in this section we load the common MNIST benchmark which we will use for our examples. We will take advantage of the *ContinualAI* calab scripts for easy loading of the MNIST images as numpy tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VdJOCNYbYu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5RB5PMdbDNx",
        "colab_type": "code",
        "outputId": "290e71bd-06eb-4454-e52e-aa8285e569ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "!git clone https://github.com/ContinualAI/colab.git continualai/colab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'continualai/colab'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 110 (delta 2), reused 9 (delta 1), pack-reused 97\u001b[K\n",
            "Receiving objects: 100% (110/110), 204.78 KiB | 835.00 KiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD0nAldMbD5O",
        "colab_type": "code",
        "outputId": "b3d7e107-1dac-4a21-9361-1f794ce0a947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from continualai.colab.scripts import mnist\n",
        "mnist.init()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz...\n",
            "Downloading t10k-images-idx3-ubyte.gz...\n",
            "Downloading train-labels-idx1-ubyte.gz...\n",
            "Downloading t10k-labels-idx1-ubyte.gz...\n",
            "Download complete.\n",
            "Save complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4cSFTbcbKeb",
        "colab_type": "code",
        "outputId": "82010608-6f23-4abd-c92f-729367f4d7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "x_train, t_train, x_test, t_test = mnist.load()\n",
        "\n",
        "print(\"x_train dim and type: \", x_train.shape, x_train.dtype)\n",
        "print(\"t_train dim and type: \", t_train.shape, t_train.dtype)\n",
        "print(\"x_test dim and type: \", x_test.shape, x_test.dtype)\n",
        "print(\"t_test dim and type: \", t_test.shape, t_test.dtype)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train dim and type:  (60000, 1, 28, 28) float32\n",
            "t_train dim and type:  (60000,) uint8\n",
            "x_test dim and type:  (10000, 1, 28, 28) float32\n",
            "t_test dim and type:  (10000,) uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h1yw26KrRUN",
        "colab_type": "text"
      },
      "source": [
        "Let us take a look at these images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1PMSnyNbNxr",
        "colab_type": "code",
        "outputId": "2a79cebd-c2c4-4e08-bda3-28719166ca82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "f, axarr = plt.subplots(2,2)\n",
        "axarr[0,0].imshow(x_train[1, 0], cmap=\"gray\")\n",
        "axarr[0,1].imshow(x_train[2, 0], cmap=\"gray\")\n",
        "axarr[1,0].imshow(x_train[3, 0], cmap=\"gray\")\n",
        "axarr[1,1].imshow(x_train[4, 0], cmap=\"gray\")\n",
        "np.vectorize(lambda ax:ax.axis('off'))(axarr);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAADnCAYAAABcxZBBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ajRYeYGBWm8",
        "colab_type": "text"
      },
      "source": [
        "## Common Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5neMaipojPP",
        "colab_type": "text"
      },
      "source": [
        "Now we can move on and define some common constants that we will share across the DL framework experiments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZwum8trBbJf",
        "colab_type": "code",
        "outputId": "80df7640-5e4f-46d7-8657-23d40cc7b039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# we will use time to measure speed\n",
        "import time\n",
        "\n",
        "# number of classes in the MNIST dataset\n",
        "num_class = 10\n",
        "\n",
        "# number of epochs we will use for each training\n",
        "n_epochs = 2\n",
        "\n",
        "# mini-batch size for SGD\n",
        "minibatch_size = 100\n",
        "\n",
        "# Iterations for epoch for the two sets\n",
        "tr_it_for_epoch = t_train.shape[0] // minibatch_size\n",
        "te_it_for_epoch = t_test.shape[0] // minibatch_size\n",
        "print(\"train iterations: \", tr_it_for_epoch)\n",
        "print(\"test iterations: \", te_it_for_epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train iterations:  600\n",
            "test iterations:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDYuMadt28ET",
        "colab_type": "text"
      },
      "source": [
        "## Training a ConvNet with Caffe\n",
        "\n",
        "Let us focus on *Caffe*. First of all let us install the library. Luckily enough for Ubuntu (>= 17.04) there is a packaged version we can install simply with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv28V1_g1jeB",
        "colab_type": "code",
        "outputId": "03ae7412-27a2-46f8-ba61-259c2b4c753b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# !apt install -y caffe-cpu\n",
        "!apt install -y caffe-cuda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 65%\r\rReading package lists... 65%\r\rReading package lists... 66%\r\rReading package lists... 66%\r\rReading package lists... 69%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 73%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  caffe-tools-cuda cython3 fonts-lyx javascript-common libblosc1\n",
            "  libcaffe-cuda1 libcublas9.1 libcudart9.1 libcurand9.1 libgflags2.2\n",
            "  libgoogle-glog0v5 libjs-jquery-ui libleveldb1v5 liblmdb0\n",
            "  python-matplotlib-data python-tables-data python3-bs4 python3-caffe-cuda\n",
            "  python3-chardet python3-cycler python3-dateutil python3-decorator\n",
            "  python3-gflags python3-h5py python3-html5lib python3-ipython\n",
            "  python3-ipython-genutils python3-leveldb python3-lxml python3-matplotlib\n",
            "  python3-networkx python3-nose python3-numexpr python3-olefile python3-pandas\n",
            "  python3-pandas-lib python3-pexpect python3-pickleshare python3-pil\n",
            "  python3-pkg-resources python3-prompt-toolkit python3-protobuf\n",
            "  python3-ptyprocess python3-pygments python3-pyparsing python3-pywt\n",
            "  python3-scipy python3-simplegeneric python3-six python3-skimage\n",
            "  python3-skimage-lib python3-tables python3-tables-lib python3-traitlets\n",
            "  python3-tz python3-wcwidth python3-webencodings python3-yaml\n",
            "  ttf-bitstream-vera\n",
            "Suggested packages:\n",
            "  libcaffe-cuda-dev caffe-doc cython-doc apache2 | lighttpd | httpd\n",
            "  libjs-jquery-ui-docs python-cycler-doc python-h5py-doc python3-genshi\n",
            "  python3-lxml-dbg python-lxml-doc dvipng gir1.2-gtk-3.0 ghostscript inkscape\n",
            "  ipython3 python-matplotlib-doc python3-cairocffi python3-gi-cairo\n",
            "  python3-gobject python3-pyqt4 python3-sip python3-tornado\n",
            "  texlive-extra-utils texlive-latex-extra ttf-staypuft python3-pydotplus\n",
            "  python-nose-doc python-pandas-doc python-pexpect-doc python-pil-doc\n",
            "  python3-pil-dbg python3-setuptools python-pyparsing-doc python-scipy-doc\n",
            "  python-skimage-doc python-tables-doc python3-netcdf4 vitables\n",
            "The following NEW packages will be installed:\n",
            "  caffe-cuda caffe-tools-cuda cython3 fonts-lyx javascript-common libblosc1\n",
            "  libcaffe-cuda1 libcublas9.1 libcudart9.1 libcurand9.1 libgflags2.2\n",
            "  libgoogle-glog0v5 libjs-jquery-ui libleveldb1v5 liblmdb0\n",
            "  python-matplotlib-data python-tables-data python3-bs4 python3-caffe-cuda\n",
            "  python3-chardet python3-cycler python3-dateutil python3-decorator\n",
            "  python3-gflags python3-h5py python3-html5lib python3-ipython\n",
            "  python3-ipython-genutils python3-leveldb python3-lxml python3-matplotlib\n",
            "  python3-networkx python3-nose python3-numexpr python3-olefile python3-pandas\n",
            "  python3-pandas-lib python3-pexpect python3-pickleshare python3-pil\n",
            "  python3-pkg-resources python3-prompt-toolkit python3-protobuf\n",
            "  python3-ptyprocess python3-pygments python3-pyparsing python3-pywt\n",
            "  python3-scipy python3-simplegeneric python3-six python3-skimage\n",
            "  python3-skimage-lib python3-tables python3-tables-lib python3-traitlets\n",
            "  python3-tz python3-wcwidth python3-webencodings python3-yaml\n",
            "  ttf-bitstream-vera\n",
            "0 upgraded, 60 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 121 MB of archives.\n",
            "After this operation, 315 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-yaml amd64 3.12-1build2 [109 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 libcublas9.1 amd64 9.1.85-3ubuntu1 [25.0 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 libcudart9.1 amd64 9.1.85-3ubuntu1 [121 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 libcurand9.1 amd64 9.1.85-3ubuntu1 [38.9 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgflags2.2 amd64 2.2.1-1 [72.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgoogle-glog0v5 amd64 0.3.5-1 [50.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libleveldb1v5 amd64 1.20-2 [136 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 liblmdb0 amd64 0.9.21-1ubuntu0.1 [44.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 libcaffe-cuda1 amd64 1.0.0-6build1 [1,600 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 caffe-tools-cuda amd64 1.0.0-6build1 [105 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cython3 amd64 0.26.1-0.4 [1,925 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-dateutil all 2.6.1-1 [52.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-gflags all 1.5.1-5 [35.6 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-h5py amd64 2.7.1-2 [631 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-decorator all 4.1.2-1 [9,364 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-ptyprocess all 0.5.2-1 [12.7 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pexpect all 4.2.1-1 [42.4 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-pickleshare all 0.7.4-2 [6,904 B]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wcwidth all 0.1.7+dfsg1-1 [14.7 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-prompt-toolkit all 1.0.15-1 [163 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pygments all 2.2.0+dfsg-1 [574 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-simplegeneric all 0.8.1-1 [11.5 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-ipython-genutils all 0.2.0-1 [20.9 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-traitlets all 4.3.2-1 [59.1 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-ipython all 5.5.0-1 [381 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-leveldb amd64 0~svn68-3build3 [18.3 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 fonts-lyx all 2.2.4-0ubuntu0.18.04.1 [155 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/universe amd64 ttf-bitstream-vera all 1.10-8 [352 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-matplotlib-data all 2.1.1-2ubuntu3 [3,774 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pyparsing all 2.2.0+dfsg1-2 [52.2 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-tz all 2018.3-2 [25.1 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libjs-jquery-ui all 1.12.1+dfsg-5 [232 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-cycler all 0.10.0-1 [7,622 B]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-matplotlib amd64 2.1.1-2ubuntu3 [3,907 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-networkx all 1.11-1ubuntu3 [606 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-nose all 1.3.7-3 [115 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-pandas-lib amd64 0.22.0-4 [3,041 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-pandas all 0.22.0-4 [2,764 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pil amd64 5.1.0-1 [328 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-protobuf amd64 3.0.0-9.1ubuntu1 [262 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-scipy amd64 0.19.1-2ubuntu1 [9,619 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-skimage-lib amd64 0.13.1-2 [1,504 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-pywt amd64 0.5.1-1.1ubuntu4 [932 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-skimage all 0.13.1-2 [19.6 MB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 python3-caffe-cuda amd64 1.0.0-6build1 [689 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 caffe-cuda amd64 1.0.0-6build1 [4,564 B]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/main amd64 javascript-common all 11 [6,066 B]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libblosc1 amd64 1.14.2+ds1-1 [31.4 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-tables-data all 3.4.2-4 [46.1 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-bs4 all 4.6.0-1 [67.8 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-chardet all 3.0.4-1 [80.3 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-webencodings all 0.5-2 [10.4 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-html5lib all 0.999999999-1 [81.9 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-lxml amd64 4.2.1-1ubuntu0.1 [1,091 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-numexpr amd64 2.6.4-1 [119 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-olefile all 0.45.1-1 [33.3 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-tables-lib amd64 3.4.2-4 [413 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-tables all 3.4.2-4 [331 kB]\n",
            "Fetched 121 MB in 2s (61.9 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package python3-yaml.\n",
            "(Reading database ... 145653 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python3-yaml_3.12-1build2_amd64.deb ...\n",
            "Unpacking python3-yaml (3.12-1build2) ...\n",
            "Selecting previously unselected package libcublas9.1:amd64.\n",
            "Preparing to unpack .../01-libcublas9.1_9.1.85-3ubuntu1_amd64.deb ...\n",
            "Unpacking libcublas9.1:amd64 (9.1.85-3ubuntu1) ...\n",
            "Selecting previously unselected package libcudart9.1:amd64.\n",
            "Preparing to unpack .../02-libcudart9.1_9.1.85-3ubuntu1_amd64.deb ...\n",
            "Unpacking libcudart9.1:amd64 (9.1.85-3ubuntu1) ...\n",
            "Selecting previously unselected package libcurand9.1:amd64.\n",
            "Preparing to unpack .../03-libcurand9.1_9.1.85-3ubuntu1_amd64.deb ...\n",
            "Unpacking libcurand9.1:amd64 (9.1.85-3ubuntu1) ...\n",
            "Selecting previously unselected package libgflags2.2.\n",
            "Preparing to unpack .../04-libgflags2.2_2.2.1-1_amd64.deb ...\n",
            "Unpacking libgflags2.2 (2.2.1-1) ...\n",
            "Selecting previously unselected package libgoogle-glog0v5.\n",
            "Preparing to unpack .../05-libgoogle-glog0v5_0.3.5-1_amd64.deb ...\n",
            "Unpacking libgoogle-glog0v5 (0.3.5-1) ...\n",
            "Selecting previously unselected package libleveldb1v5:amd64.\n",
            "Preparing to unpack .../06-libleveldb1v5_1.20-2_amd64.deb ...\n",
            "Unpacking libleveldb1v5:amd64 (1.20-2) ...\n",
            "Selecting previously unselected package liblmdb0:amd64.\n",
            "Preparing to unpack .../07-liblmdb0_0.9.21-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking liblmdb0:amd64 (0.9.21-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libcaffe-cuda1:amd64.\n",
            "Preparing to unpack .../08-libcaffe-cuda1_1.0.0-6build1_amd64.deb ...\n",
            "Unpacking libcaffe-cuda1:amd64 (1.0.0-6build1) ...\n",
            "Selecting previously unselected package caffe-tools-cuda.\n",
            "Preparing to unpack .../09-caffe-tools-cuda_1.0.0-6build1_amd64.deb ...\n",
            "Unpacking caffe-tools-cuda (1.0.0-6build1) ...\n",
            "Selecting previously unselected package cython3.\n",
            "Preparing to unpack .../10-cython3_0.26.1-0.4_amd64.deb ...\n",
            "Unpacking cython3 (0.26.1-0.4) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../11-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-dateutil.\n",
            "Preparing to unpack .../12-python3-dateutil_2.6.1-1_all.deb ...\n",
            "Unpacking python3-dateutil (2.6.1-1) ...\n",
            "Selecting previously unselected package python3-gflags.\n",
            "Preparing to unpack .../13-python3-gflags_1.5.1-5_all.deb ...\n",
            "Unpacking python3-gflags (1.5.1-5) ...\n",
            "Selecting previously unselected package python3-h5py.\n",
            "Preparing to unpack .../14-python3-h5py_2.7.1-2_amd64.deb ...\n",
            "Unpacking python3-h5py (2.7.1-2) ...\n",
            "Selecting previously unselected package python3-decorator.\n",
            "Preparing to unpack .../15-python3-decorator_4.1.2-1_all.deb ...\n",
            "Unpacking python3-decorator (4.1.2-1) ...\n",
            "Selecting previously unselected package python3-ptyprocess.\n",
            "Preparing to unpack .../16-python3-ptyprocess_0.5.2-1_all.deb ...\n",
            "Unpacking python3-ptyprocess (0.5.2-1) ...\n",
            "Selecting previously unselected package python3-pexpect.\n",
            "Preparing to unpack .../17-python3-pexpect_4.2.1-1_all.deb ...\n",
            "Unpacking python3-pexpect (4.2.1-1) ...\n",
            "Selecting previously unselected package python3-pickleshare.\n",
            "Preparing to unpack .../18-python3-pickleshare_0.7.4-2_all.deb ...\n",
            "Unpacking python3-pickleshare (0.7.4-2) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../19-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-wcwidth.\n",
            "Preparing to unpack .../20-python3-wcwidth_0.1.7+dfsg1-1_all.deb ...\n",
            "Unpacking python3-wcwidth (0.1.7+dfsg1-1) ...\n",
            "Selecting previously unselected package python3-prompt-toolkit.\n",
            "Preparing to unpack .../21-python3-prompt-toolkit_1.0.15-1_all.deb ...\n",
            "Unpacking python3-prompt-toolkit (1.0.15-1) ...\n",
            "Selecting previously unselected package python3-pygments.\n",
            "Preparing to unpack .../22-python3-pygments_2.2.0+dfsg-1_all.deb ...\n",
            "Unpacking python3-pygments (2.2.0+dfsg-1) ...\n",
            "Selecting previously unselected package python3-simplegeneric.\n",
            "Preparing to unpack .../23-python3-simplegeneric_0.8.1-1_all.deb ...\n",
            "Unpacking python3-simplegeneric (0.8.1-1) ...\n",
            "Selecting previously unselected package python3-ipython-genutils.\n",
            "Preparing to unpack .../24-python3-ipython-genutils_0.2.0-1_all.deb ...\n",
            "Unpacking python3-ipython-genutils (0.2.0-1) ...\n",
            "Selecting previously unselected package python3-traitlets.\n",
            "Preparing to unpack .../25-python3-traitlets_4.3.2-1_all.deb ...\n",
            "Unpacking python3-traitlets (4.3.2-1) ...\n",
            "Selecting previously unselected package python3-ipython.\n",
            "Preparing to unpack .../26-python3-ipython_5.5.0-1_all.deb ...\n",
            "Unpacking python3-ipython (5.5.0-1) ...\n",
            "Selecting previously unselected package python3-leveldb.\n",
            "Preparing to unpack .../27-python3-leveldb_0~svn68-3build3_amd64.deb ...\n",
            "Unpacking python3-leveldb (0~svn68-3build3) ...\n",
            "Selecting previously unselected package fonts-lyx.\n",
            "Preparing to unpack .../28-fonts-lyx_2.2.4-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking fonts-lyx (2.2.4-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package ttf-bitstream-vera.\n",
            "Preparing to unpack .../29-ttf-bitstream-vera_1.10-8_all.deb ...\n",
            "Unpacking ttf-bitstream-vera (1.10-8) ...\n",
            "Selecting previously unselected package python-matplotlib-data.\n",
            "Preparing to unpack .../30-python-matplotlib-data_2.1.1-2ubuntu3_all.deb ...\n",
            "Unpacking python-matplotlib-data (2.1.1-2ubuntu3) ...\n",
            "Selecting previously unselected package python3-pyparsing.\n",
            "Preparing to unpack .../31-python3-pyparsing_2.2.0+dfsg1-2_all.deb ...\n",
            "Unpacking python3-pyparsing (2.2.0+dfsg1-2) ...\n",
            "Selecting previously unselected package python3-tz.\n",
            "Preparing to unpack .../32-python3-tz_2018.3-2_all.deb ...\n",
            "Unpacking python3-tz (2018.3-2) ...\n",
            "Selecting previously unselected package libjs-jquery-ui.\n",
            "Preparing to unpack .../33-libjs-jquery-ui_1.12.1+dfsg-5_all.deb ...\n",
            "Unpacking libjs-jquery-ui (1.12.1+dfsg-5) ...\n",
            "Selecting previously unselected package python3-cycler.\n",
            "Preparing to unpack .../34-python3-cycler_0.10.0-1_all.deb ...\n",
            "Unpacking python3-cycler (0.10.0-1) ...\n",
            "Selecting previously unselected package python3-matplotlib.\n",
            "Preparing to unpack .../35-python3-matplotlib_2.1.1-2ubuntu3_amd64.deb ...\n",
            "Unpacking python3-matplotlib (2.1.1-2ubuntu3) ...\n",
            "Selecting previously unselected package python3-networkx.\n",
            "Preparing to unpack .../36-python3-networkx_1.11-1ubuntu3_all.deb ...\n",
            "Unpacking python3-networkx (1.11-1ubuntu3) ...\n",
            "Selecting previously unselected package python3-nose.\n",
            "Preparing to unpack .../37-python3-nose_1.3.7-3_all.deb ...\n",
            "Unpacking python3-nose (1.3.7-3) ...\n",
            "Selecting previously unselected package python3-pandas-lib.\n",
            "Preparing to unpack .../38-python3-pandas-lib_0.22.0-4_amd64.deb ...\n",
            "Unpacking python3-pandas-lib (0.22.0-4) ...\n",
            "Selecting previously unselected package python3-pandas.\n",
            "Preparing to unpack .../39-python3-pandas_0.22.0-4_all.deb ...\n",
            "Unpacking python3-pandas (0.22.0-4) ...\n",
            "Selecting previously unselected package python3-pil:amd64.\n",
            "Preparing to unpack .../40-python3-pil_5.1.0-1_amd64.deb ...\n",
            "Unpacking python3-pil:amd64 (5.1.0-1) ...\n",
            "Selecting previously unselected package python3-protobuf.\n",
            "Preparing to unpack .../41-python3-protobuf_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking python3-protobuf (3.0.0-9.1ubuntu1) ...\n",
            "Selecting previously unselected package python3-scipy.\n",
            "Preparing to unpack .../42-python3-scipy_0.19.1-2ubuntu1_amd64.deb ...\n",
            "Unpacking python3-scipy (0.19.1-2ubuntu1) ...\n",
            "Selecting previously unselected package python3-skimage-lib:amd64.\n",
            "Preparing to unpack .../43-python3-skimage-lib_0.13.1-2_amd64.deb ...\n",
            "Unpacking python3-skimage-lib:amd64 (0.13.1-2) ...\n",
            "Selecting previously unselected package python3-pywt.\n",
            "Preparing to unpack .../44-python3-pywt_0.5.1-1.1ubuntu4_amd64.deb ...\n",
            "Unpacking python3-pywt (0.5.1-1.1ubuntu4) ...\n",
            "Selecting previously unselected package python3-skimage.\n",
            "Preparing to unpack .../45-python3-skimage_0.13.1-2_all.deb ...\n",
            "Unpacking python3-skimage (0.13.1-2) ...\n",
            "Selecting previously unselected package python3-caffe-cuda.\n",
            "Preparing to unpack .../46-python3-caffe-cuda_1.0.0-6build1_amd64.deb ...\n",
            "Unpacking python3-caffe-cuda (1.0.0-6build1) ...\n",
            "Selecting previously unselected package caffe-cuda.\n",
            "Preparing to unpack .../47-caffe-cuda_1.0.0-6build1_amd64.deb ...\n",
            "Unpacking caffe-cuda (1.0.0-6build1) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../48-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libblosc1.\n",
            "Preparing to unpack .../49-libblosc1_1.14.2+ds1-1_amd64.deb ...\n",
            "Unpacking libblosc1 (1.14.2+ds1-1) ...\n",
            "Selecting previously unselected package python-tables-data.\n",
            "Preparing to unpack .../50-python-tables-data_3.4.2-4_all.deb ...\n",
            "Unpacking python-tables-data (3.4.2-4) ...\n",
            "Selecting previously unselected package python3-bs4.\n",
            "Preparing to unpack .../51-python3-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python3-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python3-chardet.\n",
            "Preparing to unpack .../52-python3-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python3-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python3-webencodings.\n",
            "Preparing to unpack .../53-python3-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python3-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python3-html5lib.\n",
            "Preparing to unpack .../54-python3-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python3-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python3-lxml:amd64.\n",
            "Preparing to unpack .../55-python3-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python3-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-numexpr.\n",
            "Preparing to unpack .../56-python3-numexpr_2.6.4-1_amd64.deb ...\n",
            "Unpacking python3-numexpr (2.6.4-1) ...\n",
            "Selecting previously unselected package python3-olefile.\n",
            "Preparing to unpack .../57-python3-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python3-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python3-tables-lib.\n",
            "Preparing to unpack .../58-python3-tables-lib_3.4.2-4_amd64.deb ...\n",
            "Unpacking python3-tables-lib (3.4.2-4) ...\n",
            "Selecting previously unselected package python3-tables.\n",
            "Preparing to unpack .../59-python3-tables_3.4.2-4_all.deb ...\n",
            "Unpacking python3-tables (3.4.2-4) ...\n",
            "Setting up python3-yaml (3.12-1build2) ...\n",
            "Setting up libblosc1 (1.14.2+ds1-1) ...\n",
            "Setting up python3-pickleshare (0.7.4-2) ...\n",
            "Setting up libgflags2.2 (2.2.1-1) ...\n",
            "Setting up python3-pil:amd64 (5.1.0-1) ...\n",
            "Setting up python3-simplegeneric (0.8.1-1) ...\n",
            "Setting up python3-webencodings (0.5-2) ...\n",
            "Setting up python3-tables-lib (3.4.2-4) ...\n",
            "Setting up liblmdb0:amd64 (0.9.21-1ubuntu0.1) ...\n",
            "Setting up python3-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python3-olefile (0.45.1-1) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up python3-pyparsing (2.2.0+dfsg1-2) ...\n",
            "Setting up python3-cycler (0.10.0-1) ...\n",
            "Setting up python-tables-data (3.4.2-4) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up python3-bs4 (4.6.0-1) ...\n",
            "Setting up python3-gflags (1.5.1-5) ...\n",
            "update-alternatives: using /usr/bin/python3-gflags2man to provide /usr/bin/gflags2man (gflags2man) in auto mode\n",
            "Setting up libleveldb1v5:amd64 (1.20-2) ...\n",
            "Setting up python3-skimage-lib:amd64 (0.13.1-2) ...\n",
            "Setting up python3-pandas-lib (0.22.0-4) ...\n",
            "Setting up python3-wcwidth (0.1.7+dfsg1-1) ...\n",
            "Setting up libcurand9.1:amd64 (9.1.85-3ubuntu1) ...\n",
            "Setting up python3-protobuf (3.0.0-9.1ubuntu1) ...\n",
            "Setting up python3-ipython-genutils (0.2.0-1) ...\n",
            "Setting up python3-nose (1.3.7-3) ...\n",
            "Setting up libgoogle-glog0v5 (0.3.5-1) ...\n",
            "Setting up python3-chardet (3.0.4-1) ...\n",
            "Setting up python3-html5lib (0.999999999-1) ...\n",
            "Setting up libjs-jquery-ui (1.12.1+dfsg-5) ...\n",
            "Setting up libcublas9.1:amd64 (9.1.85-3ubuntu1) ...\n",
            "Setting up ttf-bitstream-vera (1.10-8) ...\n",
            "Setting up cython3 (0.26.1-0.4) ...\n",
            "Setting up python3-pywt (0.5.1-1.1ubuntu4) ...\n",
            "Setting up libcudart9.1:amd64 (9.1.85-3ubuntu1) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up python3-decorator (4.1.2-1) ...\n",
            "Setting up python3-traitlets (4.3.2-1) ...\n",
            "Setting up python3-ptyprocess (0.5.2-1) ...\n",
            "Setting up python3-tz (2018.3-2) ...\n",
            "Setting up python3-leveldb (0~svn68-3build3) ...\n",
            "Setting up python3-dateutil (2.6.1-1) ...\n",
            "Setting up python3-h5py (2.7.1-2) ...\n",
            "Setting up fonts-lyx (2.2.4-0ubuntu0.18.04.1) ...\n",
            "Setting up python3-pygments (2.2.0+dfsg-1) ...\n",
            "Setting up libcaffe-cuda1:amd64 (1.0.0-6build1) ...\n",
            "Setting up python3-scipy (0.19.1-2ubuntu1) ...\n",
            "Setting up python3-prompt-toolkit (1.0.15-1) ...\n",
            "Setting up python-matplotlib-data (2.1.1-2ubuntu3) ...\n",
            "Setting up python3-numexpr (2.6.4-1) ...\n",
            "Setting up python3-tables (3.4.2-4) ...\n",
            "Setting up python3-pexpect (4.2.1-1) ...\n",
            "Setting up python3-networkx (1.11-1ubuntu3) ...\n",
            "Setting up python3-pandas (0.22.0-4) ...\n",
            "Setting up caffe-tools-cuda (1.0.0-6build1) ...\n",
            "Setting up python3-matplotlib (2.1.1-2ubuntu3) ...\n",
            "Setting up python3-ipython (5.5.0-1) ...\n",
            "Setting up python3-skimage (0.13.1-2) ...\n",
            "Setting up python3-caffe-cuda (1.0.0-6build1) ...\n",
            "Setting up caffe-cuda (1.0.0-6build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "333yU5z36lHw",
        "colab_type": "text"
      },
      "source": [
        "Let us now import the library and check the version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1BEnLBJ6NRh",
        "colab_type": "code",
        "outputId": "cd1f4936-f734-48dc-bfa3-a3843fd7e5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import caffe\n",
        "caffe.__version__\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sKZwmcTr2TO",
        "colab_type": "text"
      },
      "source": [
        "Now we can set the hardware type:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIZ-69f76pwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#caffe.set_mode_cpu()\n",
        "caffe.set_device(0)\n",
        "caffe.set_mode_gpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsLmtuS2sMd5",
        "colab_type": "text"
      },
      "source": [
        "Great! Now that we have caffe imported and configured, we can focus on the definition of our ConvNet and the training/testing procedures. The easiest way to define the network structure and the opimization parameters is to define two separate prototxts files. In this case we I have already prepared the net.prototxt and solver.prototxt files which we can import front the *ContinualAI-colab* toolchain:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UaKFXZTT1yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp continualai/colab/extras/net.prototxt .\n",
        "!cp continualai/colab/extras/solver.prototxt .\n",
        "solver_name = \"solver.prototxt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H7t8awnaax3",
        "colab_type": "text"
      },
      "source": [
        "Before moving on let's visualize them with the awesome netscope tool: \n",
        "\n",
        "*   [net.prototxt](http://ethereon.github.io/netscope/#/gist/fbc84e148391c5bd953a5ec7d613b0f9)\n",
        "*   [solver.prototxt](https://gist.github.com/vlomonaco/82dbff5eab77e146b489d27a5cd5923f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7AZnMtEw1_n",
        "colab_type": "text"
      },
      "source": [
        "Now we can define our test method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juaaT4w2Kexo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net, x, y, test_iters, test_batch_size):\n",
        "    \"\"\" test the trained net \"\"\"\n",
        "\n",
        "    acc = 0\n",
        "    loss = 0\n",
        "    for it in range(test_iters):\n",
        "        if it % 100 == 1: print(\"+\", end=\"\", flush = True)\n",
        "        start = it * test_batch_size\n",
        "        end = (it + 1) * test_batch_size\n",
        "        net.blobs['data'].data[...] = x[start:end]\n",
        "        net.blobs['label'].data[...] = y[start:end]\n",
        "        \n",
        "        blobs = net.forward([\"accuracy\", \"loss\"])\n",
        "        acc += blobs[\"accuracy\"]\n",
        "        loss += blobs[\"loss\"]\n",
        "\n",
        "    return acc / test_iters, loss / test_iters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBejsywww7MJ",
        "colab_type": "text"
      },
      "source": [
        "Load the solver and start the training procedure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nouIQYHMbo4t",
        "colab_type": "code",
        "outputId": "00c46ff9-a440-487b-989b-3215675fd200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "solver = caffe.get_solver(solver_name)\n",
        "\n",
        "t_start = time.time()\n",
        "print(\"Start Training\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  print(\"Epoch\", epoch, \" \", end=\"\")\n",
        "  for it in range(tr_it_for_epoch):\n",
        "\n",
        "    if it % 100 == 1: print(\".\", end=\"\", flush=True)   \n",
        "    start = it * minibatch_size\n",
        "    end = (it + 1) * minibatch_size\n",
        "    solver.net.blobs['data'].data[...] = x_train[start:end]\n",
        "    solver.net.blobs['label'].data[...] = t_train[start:end]\n",
        "    solver.step(1)\n",
        "\n",
        "  train_acc, train_loss = test(solver.test_nets[0], x_train, t_train,\n",
        "                     tr_it_for_epoch, minibatch_size)\n",
        "  test_acc, _ = test(solver.test_nets[0], x_test, t_test,\n",
        "                     te_it_for_epoch, minibatch_size)\n",
        "  print(\"  Train loss: %.4f  Train acc: %.2f %%  Test acc: %.2f %%\" %\n",
        "        (train_loss, train_acc * 100, test_acc * 100))\n",
        "\n",
        "t_elapsed = time.time()-t_start\n",
        "print(\"---------------------------------------------\")\n",
        "print ('%d patterns (%.2f sec.) -> %.2f patt/sec' % \n",
        "       (x_train.shape[0]*n_epochs, t_elapsed, \n",
        "        x_train.shape[0]*n_epochs / t_elapsed))\n",
        "print(\"---------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "Epoch 0  ......+++++++  Train loss: 0.0988  Train acc: 96.82 %  Test acc: 96.65 %\n",
            "Epoch 1  ......+++++++  Train loss: 0.0451  Train acc: 98.54 %  Test acc: 98.13 %\n",
            "---------------------------------------------\n",
            "120000 patterns (109.37 sec.) -> 1097.20 patt/sec\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "divl7TK36uPv",
        "colab_type": "text"
      },
      "source": [
        "### [Extra] Define model and solver from Python\n",
        "\n",
        "Of course is it possible to define the network directly in Python as shown below. However we leave this part as optional for the reader to explore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3du0nJhBFtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from caffe import layers as L\n",
        "from caffe import params as P\n",
        "from caffe.proto import caffe_pb2\n",
        "from google.protobuf import text_format\n",
        "\n",
        "def get_net(num_classes=10, train_mb_size=100):\n",
        "    \"\"\" Define net and return it as String \"\"\"\n",
        "\n",
        "    net = caffe.NetSpec()\n",
        "\n",
        "    net.data = L.Input(\n",
        "        shape=[dict(dim=[train_mb_size, 1, 28, 28, ])], ntop=1,\n",
        "        include=dict(phase=caffe.TRAIN)\n",
        "    )\n",
        "    net.test_data = L.Input(\n",
        "        shape=[dict(dim=[100, 1, 28, 28, ])], ntop=1,\n",
        "        include=dict(phase=caffe.TEST)\n",
        "    )\n",
        "    net.label = L.Input(\n",
        "        shape=[dict(dim=[train_mb_size])], ntop=1,\n",
        "        include=dict(phase=caffe.TRAIN)\n",
        "    )\n",
        "    net.test_label = L.Input(\n",
        "        shape=[dict(dim=[100])], ntop=1,\n",
        "        include=dict(phase=caffe.TEST)\n",
        "    )\n",
        "    net.conv1 = L.Convolution(\n",
        "        net.data, kernel_size=5,\n",
        "        num_output=32, param=[dict(lr_mult=1), dict(lr_mult=2)],\n",
        "        weight_filler=dict(type='xavier'),\n",
        "        bias_filler=dict(type='constant')\n",
        "    )\n",
        "    net.relu1 = L.ReLU(net.conv1, in_place=True)\n",
        "\n",
        "    net.conv2 = L.Convolution(\n",
        "        net.relu1, kernel_size=5,\n",
        "        num_output=32, param=[dict(lr_mult=1), dict(lr_mult=2)],\n",
        "        weight_filler=dict(type='xavier'),\n",
        "        bias_filler=dict(type='constant')\n",
        "    )\n",
        "    net.relu2 = L.ReLU(net.conv2, in_place=True)\n",
        "\n",
        "    net.fc1 = L.InnerProduct(\n",
        "        net.relu2, num_output=500,\n",
        "        param=[dict(lr_mult=1), dict(lr_mult=2)],\n",
        "        weight_filler=dict(type='xavier'),\n",
        "        bias_filler=dict(type='constant')\n",
        "    )\n",
        "    net.relu3 = L.ReLU(net.fc1, in_place=True)\n",
        "\n",
        "    net.out = L.InnerProduct(\n",
        "        net.fc1, num_output=num_classes,\n",
        "        param=[dict(lr_mult=1), dict(lr_mult=2)],\n",
        "        weight_filler=dict(type='xavier'),\n",
        "        bias_filler=dict(type='constant')\n",
        "    )\n",
        "\n",
        "    net.loss = L.SoftmaxWithLoss(net.out, net.label)\n",
        "    \n",
        "    net.accuracy = L.Accuracy(\n",
        "        net.out, net.test_label, include=dict(phase=caffe.TEST)\n",
        "    )\n",
        "\n",
        "    proto = str(net.to_proto())\n",
        "    proto = proto.replace('test_data', 'data').replace('test_label', 'label')\\\n",
        "        .replace('test_target', 'target')\n",
        "    \n",
        "    return proto\n",
        "\n",
        "def get_solver( net, base_lr, random_seed=1, lr_policy=\"step\", gamma=0.1,\n",
        "    stepsize=100000000, momentum=0.9, weight_decay=0.0005, test_iter=0,\n",
        "    test_interval=1000, display=20, solver_mode=caffe_pb2.SolverParameter.GPU):\n",
        "    \"\"\" Define solver and return it as String \"\"\"\n",
        "\n",
        "    solver_config = caffe_pb2.SolverParameter()\n",
        "\n",
        "    solver_config.random_seed = random_seed\n",
        "    solver_config.test_iter.append(1)\n",
        "    solver_config.test_interval = 1\n",
        "    solver_config.net = net\n",
        "    solver_config.base_lr = base_lr\n",
        "    solver_config.lr_policy = lr_policy\n",
        "    solver_config.gamma = gamma\n",
        "    solver_config.stepsize = stepsize\n",
        "    solver_config.momentum = momentum\n",
        "    solver_config.weight_decay = weight_decay\n",
        "    solver_config.snapshot_format = caffe_pb2.SolverParameter.HDF5\n",
        "    solver_config.solver_mode = solver_mode\n",
        "\n",
        "    solver_config = text_format.MessageToString(\n",
        "        solver_config, float_format='.6g'\n",
        "    )\n",
        "\n",
        "    return solver_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9B6sNMJrWz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net_name = \"net.prototxt\"\n",
        "\n",
        "with open(net_name, \"w\") as wf:\n",
        "  wf.write(get_net())\n",
        "  \n",
        "with open(solver_name, \"w\") as wf:\n",
        "  wf.write(get_solver(net_name, base_lr=0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXpmO6SvDugP",
        "colab_type": "text"
      },
      "source": [
        "Questions to explore:\n",
        "\n",
        "*   How to recover the weights of a particular layer?\n",
        "*   How to get the activations of a particular layer?\n",
        "*  How to cast a classifier into a Fully Convolutional Network?\n",
        "\n",
        "Some tips here: https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fJMJm8j9av0",
        "colab_type": "text"
      },
      "source": [
        "## Training a ConvNet with Tensorflow\n",
        "\n",
        "Let us move to the second framework we are considering: *Tensorflow*. We don't need to install it since it's already pre-loaded in Google Colaboratory (guess why! :'D). So let us start by importing it and checking the version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLf5ypBbqXC8",
        "colab_type": "code",
        "outputId": "4207ba99-bbc1-4202-94b7-c3298eb766db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW3BN4qPxvus",
        "colab_type": "text"
      },
      "source": [
        "Then we can define directly the network structure using the tf.layers API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBpfVAMncKN_",
        "colab_type": "code",
        "outputId": "6f24525c-ce22-4fdd-b2c2-b898290c3953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "x = tf.placeholder(tf.float32, shape=[minibatch_size, 1, 28, 28])\n",
        "t = tf.to_int64(tf.placeholder(tf.int32, shape=[minibatch_size]))\n",
        "\n",
        "# First Convolutional Layer\n",
        "x_image = tf.reshape(x, [-1,28,28,1])\n",
        "conv1 = tf.layers.conv2d(\n",
        "    x_image, 32, 5, strides=(1, 1), padding='valid', activation=tf.nn.relu\n",
        ")\n",
        "\n",
        "# Second Convolutional Layer\n",
        "conv2 = tf.layers.conv2d(\n",
        "    conv1, 32, 5, strides=(1, 1), padding='valid', activation=tf.nn.relu\n",
        ")\n",
        "pool2 = tf.layers.flatten(conv2)\n",
        "\n",
        "# Densely Connected Layer\n",
        "fc1 = tf.layers.dense(pool2, 500, name=\"fc1\", activation=tf.nn.relu)\n",
        "\n",
        "# Output Layer\n",
        "y_logits = tf.layers.dense(fc1, num_class, name=\"logits\")\n",
        "\n",
        "# Train and Evaluate the Model\n",
        "cross_entropy = tf.reduce_mean(\n",
        "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_logits, labels=t)\n",
        ")\n",
        "optim = tf.train.MomentumOptimizer(1e-2, momentum=0.9)\n",
        "train_step = optim.minimize(cross_entropy)\n",
        "correct_prediction = tf.equal(tf.argmax(y_logits,1), t)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "loss = tf.reduce_mean(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-fc11ea9c2119>:2: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-fc11ea9c2119>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-fc11ea9c2119>:14: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-fc11ea9c2119>:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKfPKyiIx23V",
        "colab_type": "text"
      },
      "source": [
        "As for caffe we can define the *test* function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iaAyAl5-CL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(x_set, y_set, test_iters, test_batch_size):\n",
        "    \"\"\" testing set accuracy: can be used for train and test\"\"\"\n",
        "    \n",
        "    accuracy_sum = 0.0\n",
        "    for it in range(test_iters):\n",
        "        if it % 100 == 1: print(\"+\", end=\"\", flush = True)\n",
        "        start = it * test_batch_size\n",
        "        end = (it + 1) * test_batch_size\n",
        "        accuracy_sum += sess.run(\n",
        "            fetches=accuracy,\n",
        "            feed_dict={x: x_set[start:end], t: y_set[start:end]}\n",
        "        )\n",
        "      \n",
        "    return accuracy_sum / test_iters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1wLJeyzx7sl",
        "colab_type": "text"
      },
      "source": [
        "And then we can use the computational graph just created within an interactive session:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkN-_fBuA2xY",
        "colab_type": "code",
        "outputId": "34467301-2f4f-439e-b16a-367a8cf48b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "t_start = time.time()\n",
        "print(\"Start Training\")\n",
        "train_loss = 0\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"Epoch\", epoch, \" \", end=\"\")\n",
        "    for it in range(tr_it_for_epoch):\n",
        "        if it % 100 == 1: print(\".\", end=\"\", flush = True)\n",
        "        start = it * minibatch_size\n",
        "        end = (it + 1) * minibatch_size\n",
        "        batch_loss, _ = sess.run(\n",
        "            fetches=[loss, train_step],\n",
        "            feed_dict={x: x_train[start:end], t: t_train[start:end]}\n",
        "        )\n",
        "        train_loss += batch_loss\n",
        "\n",
        "    train_loss = train_loss / tr_it_for_epoch\n",
        "    train_acc = test(\n",
        "        x_train, t_train, tr_it_for_epoch, minibatch_size,\n",
        "    )  \n",
        "    test_acc = test(\n",
        "        x_test, t_test, te_it_for_epoch, minibatch_size,\n",
        "    )\n",
        "\n",
        "    print(\"  Train loss: %.4f  Train acc: %.2f %%  Test acc: %.2f %%\" %\n",
        "        (train_loss, train_acc * 100, test_acc * 100))\n",
        "    \n",
        "t_elapsed = time.time()-t_start\n",
        "print(\"---------------------------------------------\")\n",
        "print ('%d patterns (%.2f sec.) -> %.2f patt/sec' % \n",
        "       (x_train.shape[0]*n_epochs, t_elapsed, \n",
        "        x_train.shape[0]*n_epochs / t_elapsed))\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "Epoch 0  ......+++++++  Train loss: 0.2609  Train acc: 96.40 %  Test acc: 96.23 %\n",
            "Epoch 1  ......+++++++  Train loss: 0.0572  Train acc: 98.46 %  Test acc: 98.09 %\n",
            "---------------------------------------------\n",
            "120000 patterns (24.11 sec.) -> 4977.02 patt/sec\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ATgmZZk_D1v",
        "colab_type": "text"
      },
      "source": [
        "### [Extra] Keras API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lJFRqwkyTz3",
        "colab_type": "text"
      },
      "source": [
        "As an bonus we leave to the reader the same implementation but using the keras API. Very easy, isn't it? But remember, with great abstraction power comes great responsibility..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIxSi7BG9aUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(\n",
        "      32, (5, 5), strides=(1, 1), padding='valid', activation=tf.nn.relu\n",
        "  ),\n",
        "  tf.keras.layers.Conv2D(\n",
        "      32, (5, 5), strides=(1, 1), padding='valid', activation=tf.nn.relu\n",
        "  ),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(num_class, activation=tf.nn.softmax)\n",
        "])\n",
        "optim = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
        "model.compile(optimizer=optim,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "t_start = time.time()\n",
        "print(\"Start Training\")\n",
        "\n",
        "model.fit(x_train, t_train, batch_size=minibatch_size, epochs=n_epochs)\n",
        "model.evaluate(x_test, t_test)\n",
        "\n",
        "t_elapsed = time.time()-t_start\n",
        "print(\"---------------------------------------------\")\n",
        "print ('%d patterns (%.2f sec.) -> %.2f patt/sec' % \n",
        "       (x_train.shape[0]*n_epochs, t_elapsed, \n",
        "        x_train.shape[0]*n_epochs / t_elapsed))\n",
        "print(\"---------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKALvxrjFKWp",
        "colab_type": "text"
      },
      "source": [
        "Questions to explore:\n",
        "\n",
        "*   What happens if you comment the first line of code?\n",
        "*   What if you also change the convolution padding from \"valid\" to \"same\"?\n",
        "\n",
        "Some tips here: https://keras.io/layers/convolutional/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqPN1d1B7CcH",
        "colab_type": "text"
      },
      "source": [
        "## Training a ConvNet with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlT-5HvCy6x_",
        "colab_type": "text"
      },
      "source": [
        "Let us delve now into the third and last framework we will consider: Pytorch. First of all let us install it and import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGRTfFMx7KAQ",
        "colab_type": "code",
        "outputId": "0b4af641-6e1e-4f43-ad6f-6a7091f9aa34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' #'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print('Platform:', platform, 'Accelerator:', accelerator)\n",
        "\n",
        "!pip install --upgrade --force-reinstall torchvision==0.2.0 -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl\n",
        "\n",
        "import torch\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Platform: cp36-cp36m Accelerator: cu80\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 484.0MB 28.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 17.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.0MB 436kB/s \n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.59 has requirement torch>=1.0.0, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hTorch 0.4.0 CUDA 8.0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfSqaJK57MQI",
        "colab_type": "code",
        "outputId": "d4d8d585-e340-44fb-bcae-d12e065e7047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BilxjAkF8q9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# switch to False to use CPU\n",
        "use_cuda = True\n",
        "\n",
        "use_cuda = use_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\");\n",
        "torch.manual_seed(1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR5Z55XN7Okg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY3otyjd7TXG",
        "colab_type": "text"
      },
      "source": [
        "**Questions to explore:**\n",
        "\n",
        "*   What's new in Pythorch 0.4?\n",
        "\n",
        "Some tips here: https://pytorch.org/blog/pytorch-0_4_0-migration-guide/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD96_qvWzRt-",
        "colab_type": "text"
      },
      "source": [
        "Great! So now we can define the network and the independent forward function which can dynamically change depending on the input data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2N68j127TGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(512, 500)\n",
        "        self.fc2 = nn.Linear(500, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 512)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3t-0GmG0Pfy",
        "colab_type": "text"
      },
      "source": [
        "We can define the test method as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3BjalSR7QoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, device, x_test, t_test, test_iters, test_batch_size):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for it in range(test_iters):\n",
        "        if it % 100 == 1: print(\"+\", end=\"\", flush = True)\n",
        "        start = it * test_batch_size\n",
        "        end = (it + 1) * test_batch_size\n",
        "        with torch.no_grad():\n",
        "            x = torch.from_numpy(x_test[start:end])\n",
        "            y = torch.from_numpy(t_test[start:end]).long()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output = model(x)\n",
        "            # sum up batch loss\n",
        "            test_loss += F.cross_entropy(output, y).item() \n",
        "            # get the index of the max log-probability\n",
        "            pred = output.max(1, keepdim=True)[1] \n",
        "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    return correct / len(t_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpdEII350Wg4",
        "colab_type": "text"
      },
      "source": [
        "Then finally define the model and start the training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-mvoCit7ycZ",
        "colab_type": "code",
        "outputId": "80549adf-ba87-490a-af0a-03c5dd6163eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "model.train()\n",
        "\n",
        "t_start = time.time()\n",
        "print(\"Start Training\")\n",
        "train_loss = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"Epoch\", epoch, \" \", end=\"\")\n",
        "    train_loss = 0\n",
        "    for it in range(tr_it_for_epoch):\n",
        "        if it % 100 == 1: print(\".\", end=\"\", flush = True)\n",
        "        start = it * minibatch_size\n",
        "        end = (it + 1) * minibatch_size\n",
        "        x = torch.from_numpy(x_train[start:end])\n",
        "        y = torch.from_numpy(t_train[start:end]).long()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "      \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(x)\n",
        "        loss = F.cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss\n",
        "    \n",
        "    train_loss = train_loss / tr_it_for_epoch\n",
        "    train_acc = test(\n",
        "        model, device, x_train, t_train, tr_it_for_epoch, minibatch_size,\n",
        "    )  \n",
        "    test_acc = test(\n",
        "        model, device, x_test, t_test, te_it_for_epoch, minibatch_size,\n",
        "    )\n",
        "\n",
        "    print(\"  Train loss: %.4f  Train acc: %.2f %%  Test acc: %.2f %%\" %\n",
        "        (train_loss, train_acc * 100, test_acc * 100))\n",
        "    \n",
        "t_elapsed = time.time()-t_start\n",
        "print(\"---------------------------------------------\")\n",
        "print ('%d patterns (%.2f sec.) -> %.2f patt/sec' % \n",
        "       (x_train.shape[0]*n_epochs, t_elapsed, \n",
        "        x_train.shape[0]*n_epochs / t_elapsed))\n",
        "print(\"---------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "Epoch 0  ......+++++++  Train loss: 0.3785  Train acc: 97.12 %  Test acc: 97.43 %\n",
            "Epoch 1  ......+++++++  Train loss: 0.0778  Train acc: 98.08 %  Test acc: 98.01 %\n",
            "---------------------------------------------\n",
            "120000 patterns (7.02 sec.) -> 17090.29 patt/sec\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0zANlq470iZ",
        "colab_type": "text"
      },
      "source": [
        "Wow! ~98% accuracy in such a short time.\n",
        "\n",
        "**Questions to explore:**\n",
        "\n",
        "*   Can you find a better parametrization to improve the final accuracy?\n",
        "*   Can you change the network architecture to improve the final accuracy?\n",
        "*   Can you achieve the same performances with a smaller architecture?\n",
        "*   What's the difference in accuracy if you change convolutions with fully connected layers?\n",
        "* Can you improve the speed of the training for all the frameworks described above?\n",
        "* What are the pros and cons of each framework in this simple example?\n",
        "\n",
        "Some tips here: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8t8lqRa0lFl",
        "colab_type": "text"
      },
      "source": [
        "This concludes our little tour of the thre most used open-source frameworks for Deep Learning. Please make a PR if you spot any error or you want to contribute to the **ContinualAI-Colab** project! :-) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Cpx9db8G9_",
        "colab_type": "text"
      },
      "source": [
        "**Copyright (c) 2018. ContinualAI. All rights reserved.**\n",
        "\n",
        "See the accompanying LICENSE file in the GitHub repository for terms. \n",
        "\n",
        "*Date: 27-11-2018                                                             \n",
        "Author: Vincenzo Lomonaco                                                    \n",
        "E-mail: contact@continualai.org                                           \n",
        "Website: continualai.org*              "
      ]
    }
  ]
}